{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "from scipy.optimize import minimize, Bounds\n",
    "from sympy.codegen import Print\n",
    "\n",
    "V_const = 10**14\n",
    "\n",
    "class Const:\n",
    "    #eta = 1e-3\n",
    "    #rho_f = 1000\n",
    "\n",
    "    eta = 0.32e-3 # Viscosity of acetone\n",
    "    rho_f = 790  # Density of acetone\n",
    "    # K = 10e-6\n",
    "    T = 293\n",
    "    k_b = scipy.constants.k\n",
    "    runs = 10000\n",
    "    # V = 10**6\n",
    "    BaTi_density = 4200\n",
    "\n",
    "\n",
    "high_pass = 100\n",
    "bin_number = 30\n",
    "velocity_order = 8\n",
    "filename = r\"C:\\Users\\mct2723\\Desktop\\Dielectric Data\\Third Good Set\\10_noise\"\n",
    "offset = 271\n",
    "num_files = 1\n",
    "traces_per_file = 5\n",
    "sampling_rate = 200000000\n",
    "tikonov = True\n",
    "stop = .2e-3"
   ],
   "id": "fe79415ba73a2ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def MSD_fitting_func(t,m, K, r, V):\n",
    "    trap_const = K\n",
    "    use_mass = m\n",
    "    m_f = 4/3*np.pi*r**3*Const.rho_f\n",
    "    t_k = (6 * np.pi * r * Const.eta)/trap_const\n",
    "    t_f = (Const.rho_f*r**2)/Const.eta\n",
    "    t_p = m/(6 * math.pi * r * Const.eta)\n",
    "    # find roots\n",
    "    # a * z^4 + b * z^3 + c * z^2 + d * z + e = 0\n",
    "    a = t_p\n",
    "    b = -1*np.sqrt(t_f)\n",
    "    c = 1\n",
    "    d = 0\n",
    "    e = 1 / t_k\n",
    "\n",
    "    # Coefficients array for the polynomial equation\n",
    "    coefficients = [a, b, c, d, e]\n",
    "\n",
    "    # Find the roots\n",
    "    roots = np.roots(coefficients)\n",
    "\n",
    "    term_1 = scipy.special.erfcx(roots[0]*np.sqrt(t)) / (roots[0]*(roots[0] - roots[1])*(roots[0] - roots[2])*(roots[0] - roots[3]))\n",
    "    term_2 = scipy.special.erfcx(roots[1]*np.sqrt(t)) / (roots[1]*(roots[1] - roots[0])*(roots[1] - roots[2])*(roots[1] - roots[3]))\n",
    "    term_3 = scipy.special.erfcx(roots[2]*np.sqrt(t)) / (roots[2]*(roots[2] - roots[1])*(roots[2] - roots[0])*(roots[2] - roots[3]))\n",
    "    term_4 = scipy.special.erfcx(roots[3]*np.sqrt(t)) / (roots[3]*(roots[3] - roots[1])*(roots[3] - roots[2])*(roots[3] - roots[0]))\n",
    "\n",
    "    D = Const.k_b*Const.T / (6*np.pi*Const.eta*a)\n",
    "\n",
    "    return np.real(V*(2*Const.k_b*Const.T / trap_const+ 2*Const.k_b*Const.T/(m)*(term_1+term_2+term_3+term_4)))\n",
    "\n",
    "\n",
    "def save_results_to_csv(traces, output_file):\n",
    "\n",
    "    trace_headers = [f\"Trace {i + 1}\" for i in range(len(traces))]\n",
    "\n",
    "    # Write data to CSV\n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "\n",
    "        writer.writerow(trace_headers)\n",
    "\n",
    "        max_trace_length = max(len(trace) for trace in traces)\n",
    "\n",
    "        for i in range(max_trace_length):\n",
    "            row = []\n",
    "            for trace in traces:\n",
    "                if i < len(trace):\n",
    "                    row.append(trace[i])\n",
    "                else:\n",
    "                    row.append(\"\")\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "\n",
    "def MSD_fitting_const_rho(t, MSD_data, K_guess, a_guess, V_guess, m_guess, bounds=None):\n",
    "    initial_guess = [V_guess, a_guess, K_guess]\n",
    "    def least_squares_func(x):\n",
    "        # Fit for mass only, using K and a as constants\n",
    "        K = x[2] / 10**6\n",
    "\n",
    "        V = x[0] *V_const\n",
    "        a = x[1]*10**-6\n",
    "        m = 4/3*np.pi*a**3*(Const.BaTi_density + Const.rho_f/2)\n",
    "        msd_model = MSD_fitting_func(t, m, K, a, V)\n",
    "        # Least squares: minimize the sum of squared differences\n",
    "        print_negative_indices(np.real(MSD_data), \"data\")\n",
    "            # print(f\"K: {K}, V: {V}, a: {a}, m: {m} \")\n",
    "        print_negative_indices(np.real(msd_model), \"model\")\n",
    "            # print(f\"K: {K}, V: {V}, a: {a}, m: {m} \")\n",
    "\n",
    "        return np.sum((np.log(np.real(MSD_data)) - np.log(np.real(msd_model))) ** 2)\n",
    "\n",
    "    optimal_parameters = scipy.optimize.minimize(least_squares_func, initial_guess, bounds=bounds, method=\"Nelder-Mead\", options={'maxiter':1000,'xatol': 1e-15, 'fatol': 1e-15})\n",
    "    return optimal_parameters\n",
    "\n",
    "def print_negative_indices(arr, str):\n",
    "    \"\"\"Prints the indices of negative elements in an array.\n",
    "\n",
    "    Args:\n",
    "        arr: A list of numbers.\n",
    "    \"\"\"\n",
    "    for i in range(len(arr)):\n",
    "        if arr[i] < 0:\n",
    "            print(f\"index {i} is less than zero.\" + \" \" + str)\n",
    "            return True\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from nptdms import TdmsFile\n",
    "\n",
    "def check_and_load_or_process(offset, *args,):\n",
    "    results = process_folder(offset, *args)\n",
    "    # Returns a list of args, and traces\n",
    "    return results\n",
    "\n",
    "def process_folder(offset, folder_name, data_col, num_traces, traces_per):\n",
    "    results = []\n",
    "    for i in range(num_traces):\n",
    "        print(\"Reading \", folder_name, str(i))\n",
    "        print(\"data_col \", data_col)\n",
    "        for j in range(traces_per):\n",
    "            result = process_file(folder_name, i, data_col, j, offset=offset)\n",
    "            if result:\n",
    "                results.append(result)\n",
    "    return results\n",
    "\n",
    "def process_file(folder_name, trace_num, data_col, trace_idx, offset):\n",
    "    trace_num = trace_num + offset\n",
    "    file_path = os.path.join(folder_name, \"iter_\" + str(trace_num) + \".tdms\")\n",
    "    series, args = read_tdms_file(file_path, data_col, trace_idx)\n",
    "\n",
    "    return {\n",
    "        \"series\": series,\n",
    "        \"args\": args\n",
    "    }\n",
    "def read_tdms_file(file_path, data_col, trace_idx):\n",
    "    tdms_file = TdmsFile.read(file_path)\n",
    "    sample_rate = tdms_file[\"main\"].properties.get(\"r\", None)\n",
    "    print(\"Sample rate is \" + str(sample_rate))\n",
    "    series = tdms_file[\"main\"][data_col + \"_\" + str(trace_idx)]\n",
    "    track_len = len(series.data)\n",
    "    config_args = {\n",
    "        \"sampling_rate\": sample_rate,\n",
    "        \"track_len\": track_len\n",
    "    }\n",
    "\n",
    "    return series[:], config_args\n",
    "\n",
    "def bin_data(series, bin_size):\n",
    "    # Ensuring the length of series is divisible by bin_size\n",
    "\n",
    "    length = len(series) - len(series) % bin_size\n",
    "    series = np.array(series[:length])\n",
    "    ret = np.mean(series.reshape(-1, bin_size), axis=1)\n",
    "    return ret\n",
    "\n",
    "def compute_stationary_msd(time_trace, dt):\n",
    "    n = len(time_trace)\n",
    "    msd = np.zeros(n)  # Allocate array for MSD\n",
    "    lag_times = np.arange(1, n, 1) * dt  # Calculate lag times\n",
    "\n",
    "    for tau in np.array(range(n-1))+1:\n",
    "        displacements = time_trace[tau:] - time_trace[:n - tau]\n",
    "        msd[tau] = np.mean(displacements**2)\n",
    "    msd[0] = np.average(time_trace**2)\n",
    "    return msd[1:], lag_times\n",
    "\n",
    "def compute_stationary_msd_fast(time_trace, dt):\n",
    "    n = len(time_trace)\n",
    "    msd = np.zeros(n)  # Allocate array for MSD\n",
    "    lag_times = np.arange(1, n, 1) * dt  # Calculate lag times\n",
    "\n",
    "    for tau in np.array(range(1, n, 1)):\n",
    "        displacements = time_trace[tau:] - time_trace[:n - tau]\n",
    "        msd[tau] = np.mean(displacements**2)\n",
    "    msd[0] = np.average(time_trace**2)\n",
    "    return msd[1:], lag_times\n",
    "\n",
    "\n",
    "def j_msd(signal, lags):\n",
    "    MSDs = np.zeros(len(lags))\n",
    "    ls = lags\n",
    "    for i in range(1, len(ls)):\n",
    "        counter = 0\n",
    "        #for j in range(len(signal)-ls[len(ls)-1]-1):\n",
    "        for j in range(10000):\n",
    "            MSDs[i] += (signal[j] - signal[ls[i]+j])**2\n",
    "            counter+=1\n",
    "\n",
    "        MSDs[i] /= counter\n",
    "    MSDs[0] = np.average(signal**2)\n",
    "    return MSDs\n",
    "\n",
    "def low_freq_fit(f):\n",
    "    C = 1 / (10**-6*1000)\n",
    "    x_c = 1j*f*2*np.pi\n",
    "    #A = 4000\n",
    "    A = 4.66624990e+03\n",
    "    Sallen_key = x_c**2 / (x_c**2 + A*x_c + 1.76813086e+07/3)\n",
    "    RC_high_pass = x_c / (x_c +C)\n",
    "    return Sallen_key\n",
    "\n",
    "def apply_transfer(freq, response):\n",
    "    # df = pd.read_csv(r\"C:\\Users\\mcthu\\OneDrive\\Desktop\\Lab Data\\no_filter_response.txt\")\n",
    "    # freqs = np.array(df[df.keys()[0]])\n",
    "    # print(freqs.max())\n",
    "    # print(freqs.min())\n",
    "    # r = np.array(df[df.keys()[1]])\n",
    "    # # Debug: Check for zeros in r\n",
    "    # if np.any(r == 0):\n",
    "    #     print(\"Warning: Interpolation data contains zeros in the response column.\")\n",
    "    #\n",
    "    # interpolation = scipy.interpolate.interp1d(freqs, r, fill_value = \"extrapolate\")\n",
    "\n",
    "\n",
    "    def low_freq_fit2(f):\n",
    "        C = 1 / (10**-6*1000)\n",
    "        x_c = 1j*f*2*np.pi\n",
    "        #A = 4000\n",
    "        A = 4.66624990e+03\n",
    "        Sallen_key = x_c**2 / (x_c**2 + A*x_c + 1.76813086e+07/3)\n",
    "        RC_high_pass = x_c / (x_c +C)\n",
    "        return Sallen_key\n",
    "\n",
    "    print(low_freq_fit2(freq)[low_freq_fit2(freq) <= 0])\n",
    "\n",
    "    ret = np.concatenate([[1], np.array(response[1:]) / np.sqrt(low_freq_fit2(freq)[1:])])\n",
    "    return ret\n",
    "\n"
   ],
   "id": "a30ac706c020278e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "print(\"step time is\" + str(bin_number/sampling_rate))\n",
    "\n",
    "traces = check_and_load_or_process(offset, filename, \"X\", num_files, traces_per_file)\n",
    "times = np.arange(0, len(traces[0]['series']))* (1/sampling_rate)\n",
    "\n",
    "transfered_traces = []\n",
    "\n",
    "if tikonov:\n",
    "    lda = np.abs(low_freq_fit(high_pass))\n",
    "    for trace in traces:\n",
    "        # apply the transfer and convert back\n",
    "        freq_domain_data = scipy.fft.fft(trace['series'])\n",
    "        frequencies = scipy.fft.fftfreq(len(trace['series']), times[1] - times[0])\n",
    "        transfer = low_freq_fit(frequencies)\n",
    "\n",
    "        freq_domain_data_transfer = (freq_domain_data*np.conj(transfer))/(np.real(transfer)**2 + np.imag(transfer)**2 + lda**2)\n",
    "        data = np.fft.ifft(freq_domain_data_transfer)\n",
    "\n",
    "        transfered_traces.append(np.real(data))\n",
    "else:\n",
    "    for trace in traces:\n",
    "        # apply the transfer and convert back\n",
    "        freq_domain_data = scipy.fft.fft(trace['series'])\n",
    "        frequencies = scipy.fft.fftfreq(len(trace['series']), times[1] - times[0])\n",
    "        transfer_f = apply_transfer(frequencies, freq_domain_data)\n",
    "        trace_out = np.fft.ifft(transfer_f)\n",
    "\n",
    "        transfered_traces.append(np.real(trace_out))\n",
    "\n",
    "binned_traces = []\n",
    "\n",
    "for trace in transfered_traces:\n",
    "    series = bin_data(trace, bin_number)\n",
    "    binned_traces.append(series)\n",
    "\n"
   ],
   "id": "d1ff2b0e3a810cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "time = np.arange(len(binned_traces[0]))*bin_number/sampling_rate\n",
    "\n",
    "from findiff import Diff\n",
    "\n",
    "def get_velocity_higher_order(data, time, order):\n",
    "    der = Diff(0, time[1] - time[0], acc = order)\n",
    "    return der(data)\n",
    "\n",
    "\n",
    "def get_velocity_centered_full(position, dt):\n",
    "    velocity = np.zeros_like(position)\n",
    "\n",
    "    # Use forward difference at the first point\n",
    "    velocity[0] = (position[1] - position[0]) / dt\n",
    "\n",
    "    # Use centered difference for the middle points\n",
    "    velocity[1:-1] = (position[2:] - position[:-2]) / (2 * dt)\n",
    "\n",
    "    # Use backward difference at the last point\n",
    "    velocity[-1] = (position[-1] - position[-2]) / dt\n",
    "\n",
    "    return velocity\n",
    "\n",
    "velocity_traces = []\n",
    "for trace in binned_traces:\n",
    "    # velocity = get_velocity_centered_full(trace, bin_number/sampling_rate)\n",
    "    velocity = get_velocity_higher_order(trace, time, velocity_order)\n",
    "    velocity_traces.append(velocity)\n",
    "\n",
    "plt.scatter(time[10:int(10000/bin_number)], velocity_traces[0][10:int(10000/bin_number)], label = \"velocity\", linewidth=.5, s=3)\n",
    "plt.plot(time[10:int(10000/bin_number)], velocity_traces[0][10:int(10000/bin_number)], label = \"velocity\", linewidth=.5)\n",
    "plt.legend()\n",
    "\n",
    "plt.scatter(time[10:int(10000/bin_number)], binned_traces[0][10:int(10000/bin_number)]*100000 - np.mean(binned_traces[0][10:int(10000/bin_number)]*100000), label = \"position\", linewidth=.5, s=3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# We now want to save off three files\n",
    "# The voltage to meter converted position and velocity trace, and the non voltage to meter converted position traces\n",
    "\n",
    "filenames_and_data = {f\"noise_position_bin{bin_number}\": binned_traces,\n",
    "                      f\"noise_velocity_bin{bin_number}\": velocity_traces}\n",
    "\n",
    "for filename in filenames_and_data:\n",
    "    output_path = r\"..\\for_publishing\\data\\\\\"\n",
    "    output_file = f\"{filename}.csv\"\n",
    "    file_path = output_path + output_file\n",
    "\n",
    "    save_results_to_csv(filenames_and_data[filename], file_path)"
   ],
   "id": "2738a8b718854616",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "all_msd = []\n",
    "\n",
    "for series in binned_traces:\n",
    "    print(\"trace X\")\n",
    "    # series1 = series[:int(len(series)/2)]\n",
    "    # series2 = series[int(len(series)/2):]\n",
    "    series1 = series\n",
    "    #\n",
    "    # if len(series1) != len(series2):\n",
    "    #     print(len(series1), len(series2))\n",
    "    #     print(\"lengths not equal\")\n",
    "\n",
    "    msd1, lag_times = compute_stationary_msd(series1, bin_number/sampling_rate)\n",
    "\n",
    "    final_power = 4\n",
    "    powers = np.linspace(0, final_power, 200)\n",
    "    ls = np.array( np.floor(10**powers), dtype = np.int64)\n",
    "    ls = np.unique(ls)\n",
    "    msd_j = j_msd(series1, ls)\n",
    "    plt.plot(ls * bin_number / sampling_rate, msd_j, \".\", label = \"MSD J\")\n",
    "    plt.plot(lag_times, msd1, \".\", label = \"MSD\")\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    all_msd.append(msd1)\n",
    "    #\n",
    "    # msd2, lag_times = compute_stationary_msd(series2, bin_number/sampling_rate)\n",
    "    # all_msd.append(msd2)\n",
    "\n",
    "avg_msd = np.mean(all_msd, axis=0)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(lag_times, avg_msd, linewidth=.7, linestyle=\"dashed\",label='Average MSD')\n",
    "\n",
    "for i in range(len(all_msd)):\n",
    "    plt.plot(lag_times, all_msd[i], linewidth=.7, linestyle=\"dashed\",label=f'MSD {i}')\n",
    "\n",
    "plt.legend()\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "# plt.xlim(right = 3e-6)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "97dc26a577f2c483",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# CUT THE MSD DOWN TO A REASONABLE SIZE\n",
    "max_len = int(stop*sampling_rate/bin_number)\n",
    "print(max_len)\n",
    "lag_times_ = lag_times[:max_len]\n",
    "avg_msd_ = avg_msd[:max_len]\n",
    "all_msd_ = []\n",
    "\n",
    "for i in range(len(all_msd)):\n",
    "    all_msd_.append(all_msd[i][:max_len])\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(lag_times_, avg_msd_, linewidth=.7, linestyle=\"dashed\",label='Average MSD')\n",
    "\n",
    "for i in range(len(all_msd)):\n",
    "    plt.plot(lag_times_, all_msd_[i], \".\", linewidth=.7,label=f'MSD {i}')\n",
    "\n",
    "plt.legend()\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "# plt.ylim(top=2e-4, bottom=1e-4)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "5554a664cb5e4f09",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # # FITTING THE AVERAGE\n",
    "bounds = [(1e-10, None), (1e-10, None),(1e-10, None)]\n"
   ],
   "id": "b95031309daced30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4fb6c28a76889eb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fit again without first two data points\n",
    "# WEIGHT THE HIGH FREQ DATA MORE HEAVILY ...\n",
    "\n",
    "def remove_data_points_and_first_two(lagtimes_, msd_, num_points):\n",
    "    bottom = 10\n",
    "    powers = np.linspace(1, 5, num_points)\n",
    "    ls = np.array(np.floor(10**powers), dtype=np.int64) * bin_number / sampling_rate\n",
    "    mask = ls >= lagtimes_[bottom]\n",
    "    ls_f = ls[mask]\n",
    "\n",
    "    lagtimes = np.array(lagtimes_)\n",
    "    msd = np.array(msd_)\n",
    "\n",
    "    remove = [0,1]\n",
    "    start = bottom\n",
    "\n",
    "    for i in range(len(ls_f)-1):\n",
    "        if lagtimes[start] < ls_f[i]:\n",
    "            if start > len(lagtimes) - 2:\n",
    "                break\n",
    "            start += 1\n",
    "            while start < len(lagtimes)-1 and lagtimes[start] < ls_f[i]:\n",
    "                remove.append(start)\n",
    "                start += 1\n",
    "\n",
    "    l_ret = np.delete(lagtimes, remove)\n",
    "    m_ret = np.delete(msd, remove)\n",
    "    return l_ret, m_ret\n",
    "\n",
    "down_lag_2 = 0\n",
    "all_down_msd_2 = []\n",
    "\n",
    "for msd in all_msd_:\n",
    "    down_lag_2, down_msd = remove_data_points_and_first_two(lag_times_, msd, 50)\n",
    "    all_down_msd_2.append(down_msd)\n",
    "\n",
    "# FIT AGAIN With Log sampling\n",
    "\n",
    "individual_params3 = []\n",
    "inndividual_scaled_msd3 = []\n",
    "individual_fits3 = []\n",
    "\n",
    "all_a = []\n",
    "all_v = []\n",
    "all_m = []\n",
    "all_k = []\n",
    "\n",
    "for msd in all_down_msd_2:\n",
    "    params1 = MSD_fitting_const_rho(down_lag_2, msd, 1, 3, 1, 1,  bounds = bounds)\n",
    "    p1 = params1.x\n",
    "    V1 = p1[0] *V_const\n",
    "    a1 = p1[1]*10**-6\n",
    "    m1 = 4/3*np.pi*a1**3*(Const.BaTi_density + Const.rho_f / 2)\n",
    "    K1 = p1[2] * 10**-6\n",
    "    print(f\"v is {V1}, a is {a1}, m is {m1}, K is {K1}\")\n",
    "\n",
    "    all_v.append(V1)\n",
    "    all_a.append(a1)\n",
    "    all_m.append(m1)\n",
    "    all_k.append(K1)\n",
    "\n",
    "    scaled_msd_data1 = msd/V1\n",
    "    final_msd_fit1 = MSD_fitting_func(down_lag_2, m1, K1, a1, 1)\n",
    "\n",
    "    individual_params3.append(params1)\n",
    "    individual_fits3.append(final_msd_fit1)\n",
    "    inndividual_scaled_msd3.append(scaled_msd_data1)\n",
    "\n",
    "\n",
    "for i in range(len(individual_fits3)):\n",
    "    plt.plot(down_lag_2, inndividual_scaled_msd3[i], \".\",linewidth=.6, label=f'MSD data {i}')\n",
    "    plt.plot(down_lag_2, individual_fits3[i], linewidth=.6, label=f'Model {i}')\n",
    "    print( str(individual_params3[i]))\n",
    "\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "#\n",
    "# plt.xscale(\"log\")\n",
    "# plt.yscale(\"log\")\n",
    "# plt.legend()\n",
    "plt.show()\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Needed for 3D plotting\n",
    "import numpy as np\n",
    "\n",
    "print(f\"MEAN a: {np.mean(all_a)} STD a: {np.std(all_a)} Mean k: {np.mean(all_k)} std k: {np.std(all_k)} mean V: {np.mean(all_v)} std V: {np.std(all_v)}\")\n"
   ],
   "id": "f0eafce8b17e84b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "indices_to_remove = []  # remove elements at these indices\n",
    "new_a = [x for i, x in enumerate(all_a) if i not in indices_to_remove]\n",
    "new_v = [x for i, x in enumerate(all_v) if i not in indices_to_remove]\n",
    "new_k = [x for i, x in enumerate(all_k) if i not in indices_to_remove]\n",
    "\n",
    "a_s = np.mean(new_a)\n",
    "k_s = np.mean(new_k)\n",
    "v_s = np.mean(new_v)\n",
    "\n",
    "print(f\"MEAN a: {np.mean(new_a)} Mean k: {np.mean(new_k)}  mean V: {np.mean(new_v)} \")\n",
    "print(f\" STD a: {np.std(new_a)} std k: {np.std(new_k)} std V: {np.std(new_v)}\")\n",
    "print(f\"div by 2 R {np.std(new_a)/2.0} V {np.std(new_v)/2.0} K : {np.std(new_k)/2.0}\")"
   ],
   "id": "ef3a2d6bdff2bb9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# #Remove\n",
    "# a_s = 2.675e-6\n",
    "# k_s = 1.113e-4\n",
    "# v_s = 3.473e14\n",
    "\n",
    "def PSD_standalone(omega):\n",
    "    # This is the PSD we look to fit.  We fit for 3 parameters\n",
    "    # Namely, we fit for the trap strength K, the radius of the particle a, and the voltage to position conversion V\n",
    "    mass_total = 4/3*np.pi*a_s**3*Const.BaTi_density + 2/3*np.pi*a_s**3*Const.rho_f\n",
    "    gamma_s = 6 * math.pi * a_s * Const.eta\n",
    "    tau_f = Const.rho_f * a_s ** 2 / Const.eta\n",
    "    numerator = 2 * Const.k_b * Const.T * gamma_s * (1 + np.sqrt((1 / 2) * omega * tau_f))\n",
    "    denominator = (mass_total*((k_s/mass_total)-omega**2) - omega * gamma_s * np.sqrt((1 / 2) * omega * tau_f)) ** 2 + omega ** 2 * gamma_s ** 2 * (\n",
    "            1 + np.sqrt((1 / 2) * omega * tau_f)) ** 2\n",
    "    return numerator / denominator\n",
    "\n",
    "psds = []\n",
    "frequency = 0\n",
    "for trace in binned_traces:\n",
    "    frequency, psd = scipy.signal.periodogram(trace, fs=sampling_rate/bin_number, scaling=\"density\")\n",
    "    print(sampling_rate)\n",
    "    print(bin_number)\n",
    "    psds.append(psd)\n",
    "    plt.scatter(frequency, psd, s=.1, label=\"Data PSD\", linewidth=2)\n",
    "    plt.scatter(frequency, v_s*PSD_standalone(2*math.pi*frequency), s=.1, label=\"Analytical\")\n",
    "\n",
    "    plt.ylim(bottom=1e-18)\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"PSD (m^2/Hz)\")\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.grid()\n",
    "    plt.title(\"PSD COMPARISON\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "psd = np.mean(psds, axis=0)\n",
    "\n",
    "psd_analytical = v_s*PSD_standalone(2*math.pi*frequency)\n",
    "\n",
    "plt.scatter(frequency, psd_analytical/v_s, s=.1, label=\"Analytical\")\n",
    "plt.plot(frequency, psd/v_s,  label=\"Data PSD\", linewidth=.5)\n",
    "plt.ylim(bottom=1e-30)\n",
    "plt.xlabel(\"Frequency (Hz)\")\n",
    "plt.ylabel(\"PSD (m^2/Hz)\")\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.grid()\n",
    "plt.title(\"PSD COMPARISON\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "40ecc44de962be76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mass_total = 4/3*np.pi*a_s**3*Const.BaTi_density + 2/3*np.pi*a_s**3*Const.rho_f\n",
    "vs = np.linspace(-3*10**-4, 3*10**-4, 1000)\n",
    "expected =( 1 / np.sqrt(2*np.pi) * np.sqrt(mass_total / (Const.k_b*Const.T))*np.exp(-vs**2 / (2*Const.k_b*Const.T / mass_total)))\n",
    "\n",
    "plt.hist(velocity_traces[0]/np.sqrt(v_s), bins = 100, density = True)\n",
    "# plt.hist(velocity2 / v, bins = 100, density = True)\n",
    "plt.plot(vs, expected)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ],
   "id": "96e3902f86d27018",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(f\"This leaves us with \\n a: {a_s} \\n v: {v_s} \\n k: {k_s} \\n bin_number: {bin_number} \\n sample_rate: {sampling_rate}\")",
   "id": "b5af3589353ddc4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "af = 2.6392235755368e-06\n",
    "kf = 8.061743253740993e-05\n",
    "vf = 335982165125421.5\n",
    "mf = 4/3*np.pi*af**3*Const.BaTi_density + 2/3 * np.pi*af**3 * Const.rho_f\n",
    "_gain = np.sqrt(335982165125421.5 )\n",
    "final_msd_fit = MSD_fitting_func(lag_times, mf, kf, af, 1)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(lag_times, avg_msd, label='Average MSD')\n",
    "plt.plot(lag_times, final_msd_fit*vf, label = 'final fit')\n",
    "\n",
    "difference = (avg_msd - final_msd_fit*vf)/(final_msd_fit*vf)\n",
    "\n",
    "plt.legend()\n",
    "plt.xscale(\"log\")\n",
    "# plt.yscale(\"log\")\n",
    "plt.ylabel(\"MSD (m^2)\")\n",
    "plt.xlabel(\"time (s)\")\n",
    "plt.xlim(right = 3e-6)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(lag_times, difference, label = \"difference\")\n",
    "plt.legend()\n",
    "plt.xscale(\"log\")\n",
    "# plt.yscale(\"log\")\n",
    "# plt.xlim(right = 3e-6)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "final_lags = np.column_stack((lag_times, avg_msd/vf, final_msd_fit))\n",
    "# Dont save this??\n",
    "# np.savetxt(f'../data/eq_msd.dat', final_lags, fmt='%.15e', delimiter=' ', header='#   Lags |  Eq MSD Data  |  Final Fit', comments='')"
   ],
   "id": "ff6e52c5ca87156e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
