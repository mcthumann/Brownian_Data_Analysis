{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "filename = r\"C:\\Users\\mcthu\\OneDrive\\Desktop\\Lab Data\\New_Detector_data\\use_data-selected\"\n",
    "\n",
    "offset = 100910524\n",
    "num_files = 1\n",
    "traces_per_file = 8\n",
    "\n",
    "# Found by verifying where the noise cumulative VPSD reaches 4% of signal cumulative VPSD\n",
    "bin_num = 160\n",
    "V = 357582919501679.9\n",
    "\n",
    "high_pass = 100\n"
   ],
   "id": "29fbff41a4feab48",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from nptdms import TdmsFile\n",
    "\n",
    "def save_results_to_csv(traces, output_file):\n",
    "\n",
    "    trace_headers = [f\"Trace {i + 1}\" for i in range(len(traces))]\n",
    "\n",
    "    # Write data to CSV\n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "\n",
    "        writer.writerow(trace_headers)\n",
    "\n",
    "        max_trace_length = max(len(trace) for trace in traces)\n",
    "\n",
    "        for i in range(max_trace_length):\n",
    "            row = []\n",
    "            for trace in traces:\n",
    "                if i < len(trace):\n",
    "                    row.append(trace[i])\n",
    "                else:\n",
    "                    row.append(\"\")\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"Data saved to {output_file}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def check_and_load_or_process(offset, *args,):\n",
    "    results = process_folder(offset, *args)\n",
    "    return results\n",
    "\n",
    "def process_folder(offset, folder_name, data_col, num_traces, traces_per):\n",
    "    results = []\n",
    "    for i in range(num_traces):\n",
    "        print(\"Reading \", folder_name, str(i))\n",
    "        print(\"data_col \", data_col)\n",
    "        for j in range(traces_per):\n",
    "            result = process_file(folder_name, i, data_col, j, offset=offset)\n",
    "            if result:\n",
    "                results.append(result)\n",
    "    return results\n",
    "\n",
    "def process_file(folder_name, trace_num, data_col, trace_idx, offset):\n",
    "    trace_num = trace_num + offset\n",
    "    file_path = os.path.join(folder_name, \"iter_\" + str(trace_num) + \".tdms\")\n",
    "    series, args = read_tdms_file(file_path, data_col, trace_idx)\n",
    "\n",
    "    return {\n",
    "        \"series\": series,\n",
    "        \"args\": args\n",
    "    }\n",
    "\n",
    "def read_tdms_file(file_path, data_col, trace_idx):\n",
    "    tdms_file = TdmsFile.read(file_path)\n",
    "    sample_rate = tdms_file[\"main\"].properties.get(\"r\", None)\n",
    "    series = tdms_file[\"main\"][data_col + \"_\" + str(trace_idx)]\n",
    "    track_len = len(series.data)\n",
    "    config_args = {\n",
    "        \"sampling_rate\": sample_rate,\n",
    "        \"track_len\": track_len\n",
    "    }\n",
    "\n",
    "    return series[:], config_args\n"
   ],
   "id": "ff833ceadfe2dd62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the File\n",
    "results = check_and_load_or_process(offset, filename, \"X\", num_files, traces_per_file)\n",
    "print(results[0][\"args\"])"
   ],
   "id": "10dbec1812131388",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Bin the data and set the time trace x axis\n",
    "\n",
    "def bin_data(series, bin_size):\n",
    "    # Ensuring the length of series is divisible by bin_size\n",
    "    length = len(series) - len(series) % bin_size\n",
    "    series = np.array(series[:length])\n",
    "    return np.mean(series.reshape(-1, bin_size), axis=1)\n",
    "\n",
    "traces = []\n",
    "for trace in results:\n",
    "    series = bin_data(trace[\"series\"], bin_num)\n",
    "    traces.append(series)\n",
    "\n",
    "time = np.arange(0, len(traces[0]))\n",
    "print(\"SAMPLING RATE IS \" + str(results[0]['args']['sampling_rate']))\n",
    "time = time * (bin_num/(results[0]['args']['sampling_rate']))\n",
    "fs = results[0]['args']['sampling_rate']/bin_num\n"
   ],
   "id": "4a5dbc151d430fae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Apply the transfer function\n",
    "\n",
    "# To account for near zero DC transfer we apply tikhonov regularization. We find alpha to be the frequency in which noise (low f) begins to dominate (around 100Hz)\n",
    "\n",
    "# We will appky the following to the signal S:   U = S/T    -->  U = S*T/(T^2 + L^2)      where U is the underlying signal, S is the raw signal, T is the transfer function and L is some paramter lambda\n",
    "\n",
    "def transfer_func1(f):\n",
    "    C = 1 / (10**-6*1000)\n",
    "    x_c = 1j*f*2*np.pi\n",
    "    A = 4000\n",
    "    Sallen_key = x_c**2 / (x_c**2 + A*x_c + (1/3)*(A)**2)\n",
    "    RC_high_pass = x_c / (x_c+C)\n",
    "    return Sallen_key*RC_high_pass\n",
    "\n",
    "def low_freq_fit(f):\n",
    "    C = 1 / (10**-6*1000)\n",
    "    x_c = 1j*f*2*np.pi\n",
    "    #A = 4000\n",
    "    A = 4.66255013e+03\n",
    "    Sallen_key = x_c**2 / (x_c**2 + A*x_c + 15909216.44677893/3)\n",
    "    RC_high_pass = x_c / (x_c +C)\n",
    "    return np.abs(Sallen_key)\n",
    "\n",
    "# This number (100Hz) was found by visually inspecting the PSD, observing where the low f noise takes over (100Hz), and choosing lamda to be the magnitude of the transfer function at 100Hz so that the lamda term begins to take over when f > 100 Hz\n",
    "lda = np.abs(low_freq_fit(high_pass))\n",
    "\n",
    "traces_post_transfer = []\n",
    "\n",
    "for trace in traces:\n",
    "    # apply the transfer and convert back\n",
    "    freq_domain_data = scipy.fft.fft(trace)\n",
    "    frequencies = scipy.fft.fftfreq(len(trace), time[1] - time[0])\n",
    "    transfer = low_freq_fit(frequencies)\n",
    "\n",
    "    freq_domain_data_transfer = (freq_domain_data*np.conj(transfer))/(np.real(transfer)**2 + np.imag(transfer)**2 + lda**2)\n",
    "    data = np.fft.ifft(freq_domain_data_transfer)\n",
    "\n",
    "    traces_post_transfer.append(np.real(data))\n"
   ],
   "id": "7724a6a3be1f79e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "v_squared = V\n",
    "v = math.sqrt(v_squared)\n",
    "\n",
    "traces_post_meter = []\n",
    "for trace in traces_post_transfer:\n",
    "    traces_post_meter.append(np.real(trace / v))\n",
    "\n",
    "plt.plot(time[:100], traces_post_meter[0][:100], label = \"post\", linewidth=.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "828b9cd4c9eb2640",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from findiff import Diff\n",
    "\n",
    "def get_velocity_higher_order(data, time, order):\n",
    "    der = Diff(0, time[1] - time[0], acc = order)\n",
    "    return der(data)\n",
    "\n",
    "velocity_traces = []\n",
    "for trace in traces_post_meter:\n",
    "    velocity = get_velocity_higher_order(trace, time, 2)\n",
    "    velocity_traces.append(velocity)\n",
    "\n",
    "plt.plot(time[:100], velocity_traces[0][:100], label = \"velocity\", linewidth=.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "ff70ad5fee76a4e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# We now want to save off three files\n",
    "# The voltage to meter converted position and velocity trace, and the non voltage to meter converted position traces\n",
    "\n",
    "filenames_and_data = {\"barium_titanate_in_acetone_voltages\": traces_post_transfer,\n",
    "                      \"barium_titanate_in_acetone_position\": traces_post_meter,\n",
    "                      \"barium_titanate_in_acetone_velocity\": velocity_traces}\n",
    "\n",
    "for filename in filenames_and_data:\n",
    "    output_path = r\"C:\\Users\\mcthu\\OneDrive\\Desktop\\Lab Data\\April_data\\\\\"\n",
    "    output_file = f\"{filename}.csv\"\n",
    "    file_path = output_path + output_file\n",
    "\n",
    "    save_results_to_csv(filenames_and_data[filename], file_path)"
   ],
   "id": "a43ca5b7f076593c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check our PSD\n",
    "psds = []\n",
    "frequency = 0\n",
    "for trace in traces_post_meter:\n",
    "    frequency, psd = scipy.signal.periodogram(trace, fs=fs, scaling=\"density\")\n",
    "    psds.append(psd)\n",
    "psd = np.mean(psds, axis=0)\n",
    "plt.plot(frequency, psd, \".\")\n",
    "plt.ylim(bottom=1e-28, top = 1e-18)\n",
    "# plt.xlim(left = 100)\n",
    "# plt.xlim(left=1e4)\n",
    "plt.xlabel(\"Frequency (Hz)\")\n",
    "plt.ylabel(\"PSD (m^2/Hz)\")\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.grid()\n",
    "plt.title(\"PSD\")\n",
    "plt.show()"
   ],
   "id": "d60e048443741a40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check our VPSD\n",
    "vpsds = []\n",
    "frequency = 0\n",
    "for trace in velocity_traces:\n",
    "    frequency, vpsd = scipy.signal.periodogram(trace, fs=fs, scaling=\"density\")\n",
    "    vpsds.append(vpsd)\n",
    "vpsd = np.mean(vpsds, axis=0)\n",
    "# plt.plot(frequency, vpsd, label = \"VPSD\")\n",
    "cumulative_vpsd = np.cumsum(vpsd) * (frequency[1] - frequency[0])  # assuming uniform frequency spacing\n",
    "# plt.plot(frequency, cumulative_vpsd, label=\"Cumulative VPSD\")\n",
    "plt.ylim(bottom=1e-19)\n",
    "plt.ylim(top=1e-6)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Frequency (Hz)\")\n",
    "plt.ylabel(\"VPSD (m^2 * Hz)\")\n",
    "plt.title(\"VPSD\")\n",
    "plt.grid()\n",
    "\n",
    "vpsd2 = psd*(2*math.pi*frequency)**2\n",
    "cumulative_vpsd2 = np.cumsum(vpsd2) * (frequency[1] - frequency[0])  # assuming uniform frequency spacing\n",
    "plt.plot(frequency[1:], vpsd2[1:], label = \"VPSD\")\n",
    "plt.plot(frequency[1:], cumulative_vpsd2[1:], label=\"Cumulative VPSD\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "e4366674d88d954",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "def compute_VACF_time_domain(v_series):\n",
    "    n = len(v_series)\n",
    "    vacf = np.zeros(n)\n",
    "    for lag in tqdm(range(n), desc=\"VACF Compute\"):\n",
    "        vacf[lag] = np.dot(v_series[:n - lag], v_series[lag:]) / (\n",
    "                n - lag)  # Normalize by number of overlapping terms\n",
    "    return vacf\n",
    "\n",
    "all_vacf = []\n",
    "#\n",
    "# for series in traces_post_meter:\n",
    "#     v_series = np.diff(series) / np.diff(time)\n",
    "#     vacf = compute_VACF_time_domain(v_series)\n",
    "#     all_vacf.append(vacf)\n",
    "\n",
    "for trace in velocity_traces:\n",
    "    vacf = compute_VACF_time_domain(trace)\n",
    "    all_vacf.append(vacf)"
   ],
   "id": "4aeaa6491597999e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "avg_vacf = np.mean(all_vacf, axis=0)\n",
    "\n",
    "plt.plot(time, avg_vacf, \".\")\n",
    "plt.xscale('log')\n",
    "# plt.xlim(right=1e-3)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.title(\"VACF\")\n",
    "plt.grid()"
   ],
   "id": "c6f0787ea9cd3ec7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compute_msd(time_trace, dt):\n",
    "    n = len(time_trace)\n",
    "    msd = np.zeros(n)  # Allocate array for MSD\n",
    "    lag_times = np.arange(n) * dt  # Calculate lag times\n",
    "\n",
    "    for tau in tqdm(range(n)):\n",
    "        displacements = time_trace[tau:] - time_trace[:n - tau]\n",
    "        msd[tau] = np.mean(displacements**2)\n",
    "\n",
    "    return msd, lag_times\n",
    "\n",
    "all_hp_msd = []\n",
    "lag_times = 0\n",
    "for series in traces_post_meter:\n",
    "    msd, lag_times = compute_msd(series, 1/fs)\n",
    "    all_hp_msd.append(msd)\n",
    "\n",
    "hp_avg_msd = np.mean(all_hp_msd, axis=0)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(lag_times, hp_avg_msd, \".\", label='HP Average MSD')\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"MSD (m^2)\")\n",
    "plt.title(\"MSD\")\n",
    "plt.grid(True)\n"
   ],
   "id": "81009a6c757ed1bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "#COMPARE TO SIMULATION\n",
    "# Load the CSV and split into two arrays\n",
    "df_loaded = pd.read_csv(\"msdarr.csv\")\n",
    "\n",
    "# Convert back to NumPy arrays\n",
    "times_sim = df_loaded[\"Array1\"].to_numpy()\n",
    "msd_sim = df_loaded[\"Array2\"].to_numpy()\n",
    "\n",
    "print(times_sim, msd_sim)\n",
    "\n",
    "hp_avg_msd = np.mean(all_hp_msd, axis=0)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(lag_times, hp_avg_msd, \".\", label='HP Average MSD')\n",
    "plt.plot(times_sim, msd_sim, \".\", label='SIM MSD')\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"MSD (m^2)\")\n",
    "plt.title(\"MSD\")\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ],
   "id": "9eaf723fa37cdf6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# See velocity distribution\n",
    "from scipy.stats import maxwell, norm\n",
    "from scipy import constants\n",
    "\n",
    "T = 297\n",
    "r = 2.69e-6\n",
    "m = 4200*(4/3)*np.pi*r**3 + 789*.5*(4/3)*math.pi*r**3\n",
    "\n",
    "sigma_v = np.sqrt(constants.k * T / m)\n",
    "\n",
    "v = np.linspace(-5* sigma_v, 5 * sigma_v, 500)\n",
    "\n",
    "pdf = np.sqrt(m/(2*math.pi*constants.k*T))*np.exp((-m*v**2)/(2*constants.k*T))\n",
    "\n",
    "area_pdf_velocity = np.trapz(pdf, v)  # Should be close to 1\n",
    "print(f\"Area under PDF Velocity: {area_pdf_velocity}\")\n",
    "\n",
    "counts, bin_edges = np.histogram(velocity_traces, bins=500, density=False)\n",
    "\n",
    "# Compute bin width\n",
    "bin_width = bin_edges[1] - bin_edges[0]\n",
    "\n",
    "# Normalize counts to make total area = 1\n",
    "normalized_counts = counts / (np.sum(counts) * bin_width)\n",
    "\n",
    "print(f\"Total Counts: {np.sum(counts)}\")\n",
    "print(f\"Bin Width: {bin_width}\")\n",
    "\n",
    "# Verify Histogram Area\n",
    "area_hist = np.sum(normalized_counts * bin_width)\n",
    "print(f\"Area under normalized histogram: {area_hist}\")\n",
    "\n",
    "# Plot the normalized histogram\n",
    "plt.bar(bin_edges[:-1], normalized_counts, width=bin_width, edgecolor='k', alpha=0.7)\n",
    "print(len(normalized_counts))\n",
    "plt.title(\"Velocity Counts\")\n",
    "\n",
    "plt.hist(v, weights=pdf, bins=500, histtype='step')\n",
    "plt.title(\"MBD\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(traces_post_meter, bins=500)\n",
    "plt.title(\"Position counts\")\n",
    "plt.show()"
   ],
   "id": "346e5eba299a2eb1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b4ef355cfe686080",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
